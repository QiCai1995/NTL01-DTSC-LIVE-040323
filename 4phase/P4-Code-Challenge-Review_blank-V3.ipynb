{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Phase 4 Code Challenge Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Pipelines and gridsearching\n",
    "- Ensemble Methods\n",
    "- Natural Language Processing\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T20:28:21.200374Z",
     "start_time": "2023-06-05T20:28:20.670592Z"
    }
   },
=======
   "execution_count": null,
   "metadata": {},
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-05T20:28:22.018944Z",
     "start_time": "2023-06-05T20:28:21.999942Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.call import call_on_students"
   ]
  },
  {
   "cell_type": "markdown",
=======
   "execution_count": null,
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.call import call_on_students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pipelines and Gridsearching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the benefits of using a pipline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Principal Component Analysis (PCA) is a tool for reducing the dimensionality of our data in a way that tries to preserve information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does a gridsearch achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce multi-collinearity among features.\n",
    "\n",
    "Reduce a ton of computing power/time\n",
    "\n",
    "Reduce over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set up a pipeline with a scaler and a logistic regression model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Don't worry for now about a train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "PCA uses covariance matrix, so it is a distance based metric so scaling is important before modeling!"
=======
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the data into train and test and then gridsearch over pipelines like the one you just built to find the best-performing model. Try C (inverse regularization) values of 10, 1, and 0.1. Try out the best estimator on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "Variance: You can examine the amount of variance explained by each principal component. The cumulative explained variance plot can help you visualize how much information is retained as you increase the number of components. Generally, you want to select enough components to capture a significant portion of the total variance (e.g., 80% or 90%)."
=======
    "# Your code here\n",
    "\n"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What sorts of ensembling methods have we looked at?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
   "execution_count": 4,
   "metadata": {
    "index": 17
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is random about a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call_on_students(1)\n",
    "\n",
    "Feature Scaling: PCA is sensitive to the scale of the features, so it's important to standardize the variables. Use techniques like z-score normalization (subtracting the mean and dividing by the standard deviation) to ensure all features have a similar scale. This step prevents features with larger variances from dominating the principal components.\n",
    "\n",
    "Feature Selection: If there are irrelevant or redundant features in your dataset, it's beneficial to perform feature selection before applying PCA. Removing such features can improve the performance of PCA and reduce computational complexity. You can use techniques like correlation analysis or feature importance ranking to select the most informative features.\n",
    "\n",
    "Data Centering: PCA calculates the covariance matrix, which requires the data to be centered around zero. Subtract the mean of each feature from the corresponding values to center the data.\n",
    "\n",
    "Data Dimensionality: If your dataset has a large number of features, you might consider reducing the dimensionality before applying PCA. Dimensionality reduction techniques like feature extraction or feature selection can be employed to reduce the number of features while preserving the most important information.\n",
    "\n",
    "Splitting Data: If you plan to evaluate the performance of the PCA model, split your dataset into training and testing sets. This ensures that you can assess the model's ability to generalize to unseen data.\n",
    "\n",
    "Applying PCA: After preprocessing the data, you can apply PCA to obtain the principal components. Specify the number of components you want to retain based on the methods mentioned earlier.\n",
    "\n",
    "By following these steps, you can effectively preprocess your data for PCA and ensure that you have a clean, standardized, and centered dataset ready for dimensionality reduction and further analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 18
   },
   "outputs": [],
   "source": [
    "# Code to preprocess X\n",
    "X_preprocessed = X\n",
    "\n",
    "\n",
    "\n",
    "NEED HELP"
   ]
  },
  {
=======
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What hyperparameters of a random forest might it be useful to tune? How so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Code to import, instantiate and fit a PCA object\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit_transform(X_preprocessed)"
=======
    "4. Build a random forest model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Make sure you do a train-test split!"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 23
   },
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Code here to answer the question\n",
    "pca.explained_variance_"
=======
    "# 3) Natural Language Processing"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Concepts\n",
    "\n",
    "### Some Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 24
   },
   "outputs": [],
   "source": [
    "# Each sentence is a document\n",
    "sentence_one = \"Harry Potter is the best young adult book about wizards\"\n",
    "sentence_two = \"Um, EXCUSE ME! Ever heard of Earth Sea?\"\n",
    "sentence_three = \"I only like to read non-fiction.  It makes me a better person.\"\n",
    "\n",
    "# The corpus is composed of all of the documents\n",
    "corpus = [sentence_one, sentence_two, sentence_three]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: NLP Pre-processing\n",
    "\n",
    "List at least three steps you can take to turn raw text like this into something that would be semantically valuable (aka ready to turn into numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "Initialize k amount of centroids.\n",
    "\n",
    "Keep moving the centroids around, until you minimize distance within clusters, and maximize distance between clusters.\n"
=======
    "- Lowercase (standardize case)\n",
    "- Remove stopwords (really common words that likely have no semantic value)\n",
    "- Stem or lemmatize to remove prefixes/suffixes/grammer bits\n",
    "- Remove punctuation\n",
    "- Tokenize"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Describe what vectorized text would look like as a dataframe.\n",
    "\n",
    "If you vectorize the above corpus, what would the rows and columns be in the resulting dataframe (aka document term matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "- Inertia is Cluster Sum of Squares. The elbow curve has inertia on the y-axis and numbers of clusters on the x-axis.\n",
    "The optimal k is at the elbow point, when the inertia starts decreasing more slowly.\n"
=======
    "- Columns: every word/token in the dataset/corpus\n",
    "- Rows: the documents you're vectorizing\n"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: What does TF-IDF do?\n",
    "\n",
    "Also, what does TF-IDF stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- Sihouette coefficient. It is a value from -1 to 1. The higher the sihouette coefficient the better k number.\n",
    "In a Sihuette coefficient curve, the peak coefficient is the optimal k.\n"
   ]
  },
  {
   "cell_type": "markdown",
=======
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- TF-IDF: term frequency inverse document frequency\n",
    "- TF-IDF is a vectorizer that takes into account the rarity of the words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 33
   },
   "source": [
    "## NLP in Code\n",
    "\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "index": 34
   },
   "outputs": [],
   "source": [
    "# New section, new data\n",
    "policies = pd.read_csv('data/2020_policies_feb_24.csv')\n",
    "\n",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "data = load_iris()\n",
    "X = pd.DataFrame(data['data'])\n"
=======
    "def warren_not_warren(label):\n",
    "    \n",
    "    '''Make label a binary between Elizabeth Warren\n",
    "    speeches and speeches from all other candidates'''\n",
    "    \n",
    "    if label =='warren':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "policies['candidate'] = policies['candidate'].apply(warren_not_warren)"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 35
   },
   "source": [
    "The dataframe loaded above consists of policies of 2020 Democratic presidential hopefuls. The `policy` column holds text describing the policies themselves.  The `candidate` column indicates whether it was or was not an Elizabeth Warren policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 36
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# call_on_students(1)\n"
=======
    "policies.head()"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 37
   },
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Code to preprocess the data\n",
    "# Name the processed data X_processed\n",
    "X_processed = \n"
=======
    "The documents for activity are in the `policy` column, and the target is candidate. "
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Import the Relevant Class, Then Instantiate and Fit a Count Vectorizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Import the relevent clustering algorithm\n"
=======
    "# First! Train-test split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Code here to train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(policies['policy'], policies['candidate'])"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Fit the object\n"
=======
    "# Import the relevant vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate it\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Calculate a silhouette score\n"
=======
    "# Fit it\n",
    "vectorizer.fit(X_train)"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Vectorize Your Text, Then Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "# Code here to transform train and test sets with the vectorizer\n",
    "X_tr_vec = vectorizer.transform(X_train)\n",
    "X_te_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 44
   },
   "outputs": [],
   "source": [
    "# Importing the classifier...\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "    Returns: \n",
    "    --------\n",
    "    labels: array-like object\n",
    "        Labels attribute from the clustering model\n",
    "    \"\"\"\n",
    "    # Fit the new clustering model\n",
    "    \n",
    "    # Print the silhouette score\n",
    "    \n",
    "    # Return the labels attribute from the fit clustering model\n",
    "    \n",
    "    pass\n",
    "\n"
=======
    "# Code here to instantiate and fit a Random Forest model\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_tr_vec, y_train)"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# Testing your function\n",
    "\n",
    "for n in range(2, 9):\n",
    "    test_n_for_clustering(n, X_processed)\n",
    "    \n",
    "    \n",
    "    "
=======
    "# Code here to evaluate your model on the test set\n",
    "rfc.score(X_te_vec, y_test)"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "# 4) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
<<<<<<< HEAD:4phase/P4-Code-Challenge-Review_blank.ipynb
    "# New dataset for this section!\n",
    "ap = pd.read_csv('data/AirPassengers.csv')\n",
    "ap.head()\n"
=======
    "## Clustering Concepts"
>>>>>>> upstream/main:4phase/P4-Code-Challenge-Review_blank-V3.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 1: Describe how the K-Means algorithm updates its cluster centers after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- You set the number of cluster centers (K) - algorithm randomly starts with that number of cluster centers (in random spots!)\n",
    "- The algorithm calculates the distance between the centers and each observation and assigns the observation to the closest cluster center to create the first iteration of clusters\n",
    "- The algorithm then takes all the observations assigned to each cluster, and moves that cluster center to be at the exact actual center (mean) of the newly created cluster\n",
    "- Repeat! Until the cluster centers stop moving (or tolerance is met - some parameters in the implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 2: What is inertia, and how does K-Means use inertia to determine the best estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Please also describe the method you can use to evaluate clustering using inertia.\n",
    "\n",
    "Documentation, for reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "- Inertia measures the distance between each point and its center - the idea is that better clusters are more tightly concentrated\n",
    "- KMeans tries to minimize inertia when choosing cluster centers\n",
    "- Method to evaluate - elbow plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 3: What other metric do we have to score the clusters which are formed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Describe the difference between it and inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "- Silhouette score\n",
    "- Difference between silhouette score and inertia: silhouette score tries to maximize similarity within groups and maximize distances between clusters, while inertia just looks within each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering in Code with Heirarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "After the above conceptual review of KMeans, let's practice coding with agglomerative clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# New dataset for this section!\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 4: Prepare our Data for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "What steps do we need to take to preprocess our data effectively?\n",
    "\n",
    "- scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Code to preprocess the data\n",
    "k_scaler = StandardScaler()\n",
    "\n",
    "# Name the processed data X_processed\n",
    "X_processed = k_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 5: Import the Relevant Class, Then Instantiate and Fit a Hierarchical Agglomerative Clustering Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Let's use `n_clusters = 2` to start (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Import the relevent clustering algorithm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Instantiate\n",
    "cluster = AgglomerativeClustering(n_clusters=2)\n",
    "# Fit the object\n",
    "cluster.fit(X_processed)\n",
    "\n",
    "# Calculate a silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_processed, cluster.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 6: Write a Function to Test Different Options for `n_clusters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "The function should take in the number for `n_clusters` and the data to cluster, fit a new clustering model using that parameter to the data, print the silhouette score, then return the labels attribute from the fit clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "def test_n_for_clustering(n, data):\n",
    "    \"\"\" \n",
    "    Tests different numbers for the hyperparameter n_clusters\n",
    "    Prints the silhouette score for that clustering model\n",
    "    Returns the labels that are output from the clustering model\n",
    "\n",
    "    Parameters: \n",
    "    -----------\n",
    "    n: float object\n",
    "        number of clusters to use in the agglomerative clustering model\n",
    "    data: Pandas DataFrame or array-like object\n",
    "        Data to cluster\n",
    "\n",
    "    Returns: \n",
    "    --------\n",
    "    labels: array-like object\n",
    "        Labels attribute from the clustering model\n",
    "    \"\"\"\n",
    "    # Create the new clustering model\n",
    "    cluster = AgglomerativeClustering(n_clusters=n)\n",
    "    \n",
    "    # Fit the new clustering model\n",
    "    cluster.fit(data)\n",
    "\n",
    "    # Print the silhouette score\n",
    "    print(silhouette_score(data, cluster.labels_))\n",
    "    \n",
    "    # Return the labels attribute from the fit clustering model\n",
    "    return cluster.labels_\n",
    "\n",
    "# Testing your function\n",
    "\n",
    "for n in range(2, 9):\n",
    "    test_n_for_clustering(n, X_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
